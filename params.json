{"name":"salmonJS","tagline":"Web Crawler in Node.js to spider dynamically whole websites.","body":"# salmonJS\r\n\r\n[![Build Status](https://travis-ci.org/fabiocicerchia/salmonjs.png)](https://travis-ci.org/fabiocicerchia/salmonjs)\r\n[![Dependency Status](https://gemnasium.com/fabiocicerchia/salmonjs.png)](https://gemnasium.com/fabiocicerchia/salmonjs)\r\n[![Coverage Status](https://coveralls.io/repos/fabiocicerchia/salmonjs/badge.png)](https://coveralls.io/r/fabiocicerchia/salmonjs)\r\n[![Code Climate](https://codeclimate.com/github/fabiocicerchia/salmonjs.png)](https://codeclimate.com/github/fabiocicerchia/salmonjs)\r\n[![Bitdeli Badge](https://d2weczhvl823v0.cloudfront.net/fabiocicerchia/salmonjs/trend.png)](https://bitdeli.com/free \"Bitdeli Badge\")\r\n\r\n[![NPM](https://nodei.co/npm/salmonjs.png?downloads=true&stars=true)](https://nodei.co/npm/salmonjs/)\r\n\r\n[![salmonJS - Web Crawler in Node.js to spider dynamically whole websites.](http://jpillora.com/github-twitter-button/img/tweet.png)](https://twitter.com/intent/tweet?text=salmonJS+-+Web+Crawler+in+Node.js+to+spider+dynamically+whole+websites.&url=https%3A%2F%2Ffabiocicerchia.github.io%2Fsalmonjs&hashtags=salmonjs&original_referer=http%3A%2F%2Fgithub.com%2F&tw_p=tweetbutton)\r\n\r\nWeb Crawler in Node.js to spider dynamically whole websites.\r\n\r\n**IMPORTANT: This is a DEVELOPMENT tool, therefore SHOULD NOT be used against a\r\nwebsite you DO NOT OWN!**\r\n\r\nIt helps you to map / process entire websites, spidering them and parsing each\r\npage in a smart way. It follows all the links and test several times the form\r\nobjects. In this way is possible to check effectively the whole website.\r\n\r\n## What's this for?\r\n\r\nThis project was born with the aim of improve the legacy code, but it's not\r\nstrictly restricted only to that.\r\n\r\nsalmonJS will crawl every page from an entry-point URL, retrieving all the links\r\nin the page and firing all the events bound to any DOM element in the page in\r\norder to process all the possible combination automatically.\r\nThe only \"limitation\" of an automatic robot is the user input, so for that cases\r\nhas been implemented the test case files where it's possible to define custom\r\ninput values (e.g.: POST variables for forms, input values for javascript\r\nprompts, etc).\r\n\r\nWith this in mind the usage of salmonJS could be different based on your own\r\nneeds, like checking legacy code for dead code or profiling the web app\r\nperformance.\r\n\r\nHere below few suggestions about its usage:\r\n\r\n * Improve the legacy code\r\n  * Check the dead code (enabling the code coverage server-side)\r\n  * Discover 500 Internal Server Errors\r\n  * Discover notices and warnings\r\n  * SQL profiling\r\n * Testing\r\n  * Process forms (it'll create easy test cases to be manually compiled)\r\n  * Process automatically JS events attached to DOM nodes\r\n * Scraping\r\n  * Get the page content for each URL\r\n  * Get the screenshot for each URL\r\n * Enumeration\r\n  * URLs list\r\n  * Execution times\r\n  * Page output\r\n  * Page load\r\n * ...\r\n\r\n## Features\r\n\r\n * Command Line Interface\r\n * Catch and handle all the events bound to DOM elements (regardless how they have been set)\r\n * Follows any 3xx redirect, JS document.location and meta redirect (can be disabled)\r\n * Ignore duplicated URLs / requests and external URLs\r\n * HTTP authentication\r\n * Generate report for each page crawled, with: 6\r\n  * Screenshot\r\n  * HTTP headers\r\n  * HTTP method\r\n  * Data sent (GET and POST)\r\n  * Page output\r\n  * Execution time\r\n  * Console messages\r\n  * Alerts, Confirmations & Prompts\r\n  * Errors\r\n  * List of successful and failed requests\r\n * Multiple crawlers working asynchronously one URL each one\r\n * Support for the following HTML tags:\r\n   a, area, base, form, frame, iframe, img, input, link, script\r\n * URL normalisation\r\n * Process the web page using PhantomJS\r\n * Processing the output content only if it's HTML\r\n\r\n## Dependencies\r\n\r\nHere the list of main dependencies:\r\n\r\n * [Node.js](http://nodejs.org/download/)\r\n * [PhantomJS](http://phantomjs.org/download.html)\r\n * [CasperJS](http://casperjs.org/) (optional, only for tests)\r\n * [Redis](http://redis.io/download)\r\n\r\n## Installation\r\n\r\nYou can install it directly from npm:\r\n\r\n```\r\n[user@hostname ~]$ npm install salmonjs -g\r\n```\r\n\r\nor you can download the source code from GitHub and run these commands:\r\n\r\n```\r\n[user@hostname ~/salmonjs]$ npm install\r\n```\r\n\r\n## Configuration\r\n\r\nChange the file `src/config.js` accordingly to your needs.\r\n\r\n### Test Cases\r\n\r\nHere an example of a test case file:\r\n\r\n```\r\n; Test Case File\r\n; generated by salmonJS v0.3.0 (http://fabiocicerchia.github.io/salmonjs) at Sat, 01 Jan 1970 00:00:00 GMT\r\n; url = http://www.example.com\r\n; id = http___www_example_com\r\n\r\n[GET]\r\nvariable1=value1\r\n\r\n[POST]\r\nvariable1=value1\r\nvariable2=value2\r\nvariable3=@/path/to/file.ext ; use @ in front to use the upload feature (the file MUST exists)\r\n\r\n[COOKIE]\r\nname=value\r\n\r\n[HTTP_HEADERS]\r\nheader=value\r\n\r\n[CONFIRM]\r\nMessage=true ; true = OK, false = Cancel\r\n\r\n[PROMPT]\r\nQuestion=\"Answer\"\r\n```\r\n\r\n## Usage\r\n\r\n```\r\n              __                         _____ _______\r\n.-----.---.-.|  |.--------.-----.-----._|     |     __|\r\n|__ --|  _  ||  ||        |  _  |     |       |__     |\r\n|_____|___._||__||__|__|__|_____|__|__|_______|_______|\r\n\r\nsalmonJS v0.3.0\r\n\r\nCopyright (C) 2013 Fabio Cicerchia <info@fabiocicerchia.it>\r\n\r\nWeb Crawler in Node.js to spider dynamically whole websites.\r\nUsage: ./bin/salmonjs\r\n\r\nOptions:\r\n  --uri            The URI to be crawled                 [required]\r\n  -u, --username   Username for HTTP authentication\r\n  -p, --password   Password for HTTP authentication\r\n  -d, --details    Store details for each page           [default: false]\r\n  -f, --follow     Follows redirects                     [default: false]\r\n  --disable-stats  Disable anonymous report usage stats  [default: false]\r\n  --help           Show the help\r\n```\r\n\r\n## Examples\r\n\r\n```\r\n[user@hostname ~]$ salmonjs --uri \"http://www.google.com\"\r\n[user@hostname ~]$ salmonjs --uri \"www.google.com\"\r\n[user@hostname ~]$ salmonjs --uri \"/tmp/file.html\"\r\n[user@hostname ~]$ salmonjs --uri \"file.html\"\r\n```\r\n\r\n## Tests\r\n\r\n```\r\n[user@hostname ~/salmonjs]$ npm test\r\n```\r\n\r\n## How it works\r\n\r\n * Start processing an URL\r\n * Open a system process to PhantomJS\r\n  * Open the URL\r\n  * If there is a JS event, put it into a dedicate stack\r\n  * Inject custom event listener\r\n    * Override existent event listener\r\n  * Collect all the relevant info from the page for the report\r\n  * On load complete, execute the events in the stack\r\n  * Start to process the web page\r\n  * Get all the links from the page content\r\n  * Normalise and filter by uniqueness all the URLs collected\r\n  * Get all the JS events bound to DOM elements\r\n  * Clone the web page for each new combination in the page (confirm)\r\n  * Put the web page instance in a dedicate stack for each JS event\r\n  * Process the all the web pages in the stack\r\n  * Get all the links from the page content\r\n  * Reiterate until there are no more JS events\r\n * If there is an error retry up to 5 times\r\n * Collect all the data sent by the parser\r\n * Create test cases for POST data with normalised fields\r\n * Get POST test cases for current URL\r\n * Launch a new crawler for each test case\r\n * Store details in report file\r\n * Increase the counter for possible crawlers to be launched based on the links\r\n * Check the links if are already been processed\r\n  * If not, launch a new process for each link\r\n * If there are no more links to be processed, check if there are still sub-crawlers running\r\n  * If not so, terminate the process\r\n\r\n## Bugs\r\n\r\nFor a list of bugs please go to the [GitHub Issue Page](https://github.com/fabiocicerchia/salmonjs/issues?labels=Bug&page=1&state=open).\r\n\r\n## Licence\r\n\r\nCopyright (C) 2013 Fabio Cicerchia <info@fabiocicerchia.it>\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of\r\nthis software and associated documentation files (the \"Software\"), to deal in\r\nthe Software without restriction, including without limitation the rights to\r\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\r\nthe Software, and to permit persons to whom the Software is furnished to do so,\r\nsubject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all\r\ncopies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\r\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\r\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\r\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\r\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\r\n","google":"UA-439670-13","note":"Don't delete this file! It's used internally to help with page regeneration."}